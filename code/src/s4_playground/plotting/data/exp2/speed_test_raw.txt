Tue Apr 30 08:54:07 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  |   00000000:86:00.0 Off |                    0 |
| N/A   25C    P0             30W /  250W |       0MiB /  40960MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/zhome/c0/5/138256/Desktop/12/code/src
IMDB char level | min_freq 15 | vocab size 120
SPEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEED
SPEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEED
SPEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEED
SPEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEED
SPEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEED
datasets: ['CIFAR10']
models: ['MambaModel', 'MambaModel', 'MambaModel', 'MambaModel', 'MambaModel', 'MambaModel', 'MambaModel', 'MambaModel', 'S4ClassicModel', 'S4ClassicModel', 'S4ClassicModel']
using pos all embeddings
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
####################################################################################
MODEL: MambaModel_s6_['all', 10000, 1024, False]
trainable: 0.586m,
n_layers: 6, d_model: 116, d_state: 16
far/back mem GB: 0.5, 3.0
far/back speed b/s: 70.4, 13.2
estimated training time: 13m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 66.3
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
using pos all embeddings
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
pos_emb kw_args dict must contrain b_c_dt_x entry!
not using posembs only non-fused model
####################################################################################
MODEL: MambaModel_s6_['all', 10000, 1024, False]
trainable: 0.586m,
n_layers: 6, d_model: 116, d_state: 16
far/back mem GB: 0.5, 2.9
far/back speed b/s: 70.3, 17.1
estimated training time: 10m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 66.8
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
using pos all embeddings
using pos embeddings at b
using pos embeddings at c
using pos embeddings at dt
####################################################################################
MODEL: MambaModel_s6_['all', 10000, 1024, False, 'b_c_dt']
trainable: 0.586m,
n_layers: 6, d_model: 116, d_state: 16
Non-trainbale params: 696
far/back mem GB: 0.6, 3.3
far/back speed b/s: 47.4, 13.9
estimated training time: 12m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 66.7
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
using pos all embeddings
using pos embeddings at b
using pos embeddings at c
using pos embeddings at dt
####################################################################################
MODEL: MambaModel_s6_['all', 10000, 1024, True, 'b_c_dt']
trainable: 0.586m,
n_layers: 6, d_model: 116, d_state: 16
far/back mem GB: 0.6, 3.6
far/back speed b/s: 47.1, 12.9
estimated training time: 13m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 67.0
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
####################################################################################
MODEL: MambaModel_s6
trainable: 0.586m,
n_layers: 6, d_model: 116, d_state: 16
far/back mem GB: 0.5, 2.2
far/back speed b/s: 72.6, 18.8
estimated training time: 9m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 67.0
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
using pos all embddings
####################################################################################
MODEL: MambaModel_diag_['all', 10000, 1024, False, 'b_c_dt']
trainable: 0.563m,
n_layers: 6, d_model: 116, d_state: 16
Non-trainbale params: 696
far/back mem GB: 0.9, 5.0
far/back speed b/s: 42.8, 11.9
estimated training time: 14m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 68.0
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
using pos all embddings
####################################################################################
MODEL: MambaModel_diag_['all', 10000, 1024, True, 'b_c_dt']
trainable: 0.564m,
n_layers: 6, d_model: 116, d_state: 16
far/back mem GB: 0.9, 5.8
far/back speed b/s: 42.7, 11.4
estimated training time: 15m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 67.9
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
####################################################################################
MODEL: MambaModel_diag
trainable: 0.563m,
n_layers: 6, d_model: 116, d_state: 16
far/back mem GB: 0.9, 5.0
far/back speed b/s: 63.9, 14.9
estimated training time: 11m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 68.0
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
using pos all
####################################################################################
MODEL: S4ClassicModel_diag_['all', 10000, 1024, False]
trainable: 0.551m,
n_layers: 6, d_model: 170, d_state: 64
Non-trainbale params: 510
far/back mem GB: 0.5, 3.1
far/back speed b/s: 51.1, 16.4
estimated training time: 10m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 66.7
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
using pos all
####################################################################################
MODEL: S4ClassicModel_diag_['all', 10000, 1024, True]
trainable: 0.552m,
n_layers: 6, d_model: 170, d_state: 64
far/back mem GB: 0.5, 3.6
far/back speed b/s: 51.0, 17.3
estimated training time: 10m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 61.2
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
####################################################################################
MODEL: S4ClassicModel_diag
trainable: 0.551m,
n_layers: 6, d_model: 170, d_state: 64
far/back mem GB: 0.5, 3.0
far/back speed b/s: 64.0, 22.1
estimated training time: 7m
MANYREPS! of model throughput
DATA: CIFAR10cons
loader speed (size=64): b/s: 78.0
hparams: e:15, b:64, lr:0.001, w_d:0.01, L:1024, drop:0.1
TESTOVERBREAKING!

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 21657372: <SPEED> in cluster <dcc> Done

Job <SPEED> was submitted from host <n-62-20-1> by user <s183918> in cluster <dcc> at Tue Apr 30 06:34:51 2024
Job was executed on host(s) <4*n-62-12-23>, in queue <gpua100>, as user <s183918> in cluster <dcc> at Tue Apr 30 08:53:56 2024
</zhome/c0/5/138256> was used as the home directory.
</zhome/c0/5/138256/Desktop/12/code/src> was used as the working directory.
Started at Tue Apr 30 08:53:56 2024
Terminated at Tue Apr 30 08:56:50 2024
Results reported at Tue Apr 30 08:56:50 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
### General options
### â€“- specify queue --
#BSUB -q gpua100
#BSUB -J SPEED
### -- ask for number of cores (default: 1) --
#BSUB -n 4
### -- Select the resources: 1 gpu in exclusive process mode --
#BSUB -gpu "num=1:mode=exclusive_process"
### -- set walltime limit: hh:mm --  maximum 24 hours for GPU-queues right now
#BSUB -W 00:10
# request 5GB of system-memory
#BSUB -R "rusage[mem=10GB]"
### -- set the email address --
# please uncomment the following line and put in your e-mail address,
# if you want to receive e-mail notifications on a non-default address

### -- Specify the output and error file. %J is the job-id --
### -- -o and -e mean append, -oo and -eo mean overwrite --
#BSUB -o gpu_%J.out
#BSUB -e gpu_%J.err
# -- end of LSF options --

nvidia-smi
# Load the cuda module
module load python3/3.10.11
module load cuda/11.7

pwd

source ../12venv/bin/activate

python s4_playground/LRA_all2.py




------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   138.56 sec.
    Max Memory :                                 3131 MB
    Average Memory :                             2160.00 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               37829.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   175 sec.
    Turnaround time :                            8519 sec.

The output (if any) is above this job summary.



PS:

Read file <gpu_21657372.err> for stderr output of this job.